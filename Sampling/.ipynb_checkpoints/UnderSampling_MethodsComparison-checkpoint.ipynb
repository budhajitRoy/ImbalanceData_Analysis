{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893b12b2",
   "metadata": {},
   "source": [
    "## Compare different Under Sampling using Different Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d595d",
   "metadata": {},
   "source": [
    "- In this notebook, we will try to compare different under sampling methods on different datasets using a Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ab0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for counting\n",
    "from collections import Counter\n",
    "\n",
    "# for plotting \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from sklearn \n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# from imblearn\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.under_sampling import (\n",
    "    RandomUnderSampler,\n",
    "    CondensedNearestNeighbour,\n",
    "    EditedNearestNeighbours,\n",
    "    RepeatedEditedNearestNeighbours,\n",
    "    TomekLinks,\n",
    "    AllKNN,\n",
    "    NeighbourhoodCleaningRule,\n",
    "    NearMiss,\n",
    "    OneSidedSelection,\n",
    "    InstanceHardnessThreshold\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1c78b",
   "metadata": {},
   "source": [
    "## Initialize under sampling methods\n",
    "\n",
    "- Lets create a dictionary that contains the initialization of all our Under Sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e17a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler_dict = {\n",
    "    \n",
    "    'random' : RandomUnderSampler(\n",
    "        sampling_strategy='auto',\n",
    "        random_state=0,\n",
    "        replacement=True),\n",
    "    \n",
    "    'cnn' : CondensedNearestNeighbour(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=1,\n",
    "        random_state=0,n_jobs=2),\n",
    "    \n",
    "    'enn' : EditedNearestNeighbours(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=2),\n",
    "  \n",
    "    'renn' : RepeatedEditedNearestNeighbours(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=2,\n",
    "        max_iter=100),\n",
    "  \n",
    "    'tomek' : TomekLinks(\n",
    "        sampling_strategy='atuo',\n",
    "        n_jobs=2),\n",
    "    \n",
    "    'allknn' : AllKNN(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=2),\n",
    "    \n",
    "    'ncr' : NeighbourhoodCleaningRule(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=2,\n",
    "        threshold_cleaning=0.5),\n",
    "    \n",
    "    'oss' : OneSidedSelection(\n",
    "                            sampling_strategy='auto',\n",
    "                            random_state=0,\n",
    "                            n_neighbors=1,\n",
    "                            n_jobs=2),\n",
    "    \n",
    "    \n",
    "    'ncr' : NeighbourhoodCleaningRule(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=2,\n",
    "        threshold_cleaning=0.5),\n",
    "    \n",
    "    'oss' : OneSidedSelection(\n",
    "                            sampling_strategy='auto',\n",
    "                            random_state=0,\n",
    "                            n_neighbors=1,\n",
    "                            n_jobs=2),\n",
    "    \n",
    "    'nm1' : NearMiss(sampling_strategy='auto',\n",
    "                     version=1,\n",
    "                     n_neighbors=3,\n",
    "                     n_jobs=2),\n",
    "    \n",
    "    'nm2' : NearMiss(sampling_strategy='auto',\n",
    "                     version=2,\n",
    "                     n_neighbors=3,\n",
    "                     n_jobs=2),\n",
    "    \n",
    "\n",
    "    'iht' : InstanceHardnessThreshold(\n",
    "        estimator=LogisticRegression(max_iter=200, n_jobs=2,random_state=0), \n",
    "                                     sampling_strategy='auto',\n",
    "                                     cv=3,\n",
    "                                     random_state=0,\n",
    "                                     n_jobs=2)  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "004ed8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets define the datasets to use - as a list of datasets names\n",
    "datasets_ls = [\n",
    "    'car_eval_34',\n",
    "    'ecoli',\n",
    "    'thyroid_sick',\n",
    "    'arrhythmia',\n",
    "    'ozone_level'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b953d29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[0.49, 0.29, 0.48, ..., 0.56, 0.24, 0.35],\n",
       "        [0.07, 0.4 , 0.48, ..., 0.54, 0.35, 0.44],\n",
       "        [0.56, 0.4 , 0.48, ..., 0.49, 0.37, 0.46],\n",
       "        ...,\n",
       "        [0.61, 0.6 , 0.48, ..., 0.44, 0.39, 0.38],\n",
       "        [0.59, 0.61, 0.48, ..., 0.42, 0.42, 0.37],\n",
       "        [0.74, 0.74, 0.48, ..., 0.31, 0.53, 0.52]]),\n",
       " 'target': array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1], dtype=int64),\n",
       " 'DESCR': 'ecoli'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the datasets\n",
    "\n",
    "data = fetch_datasets()['ecoli']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d993d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 7)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(data.data[0][:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab649e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    301\n",
       " 1     35\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(data.target).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37369009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_eval_34\n",
      "-1    1594\n",
      " 1     134\n",
      "dtype: int64\n",
      "ecoli\n",
      "-1    301\n",
      " 1     35\n",
      "dtype: int64\n",
      "thyroid_sick\n",
      "-1    3541\n",
      " 1     231\n",
      "dtype: int64\n",
      "arrhythmia\n",
      "-1    427\n",
      " 1     25\n",
      "dtype: int64\n",
      "ozone_level\n",
      "-1    2463\n",
      " 1      73\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# lets try to check the data imbalance for all the data in our dataset list\n",
    "\n",
    "for dataset in datasets_ls:\n",
    "    data = fetch_datasets()[dataset]\n",
    "    print(data.DESCR)\n",
    "    data = pd.Series(data.target)\n",
    "    print(data.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7ba2c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_eval_34\n",
      "Counter({-1: 1594, 1: 134})\n",
      "\n",
      "ecoli\n",
      "Counter({-1: 301, 1: 35})\n",
      "\n",
      "thyroid_sick\n",
      "Counter({-1: 3541, 1: 231})\n",
      "\n",
      "arrhythmia\n",
      "Counter({-1: 427, 1: 25})\n",
      "\n",
      "ozone_level\n",
      "Counter({-1: 2463, 1: 73})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can do this way as well\n",
    "for dataset in datasets_ls:\n",
    "    data = fetch_datasets()[dataset]\n",
    "    print(dataset)\n",
    "    print(Counter(data.target))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "172317dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a function to train random forest model\n",
    "\n",
    "def Random_Forest(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, n_jobs=2, max_depth=4, random_state=39)\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    probs_train = rf.predict_proba(X_train)\n",
    "    probs_test = rf.predict_proba(X_test)\n",
    "    \n",
    "    print('ROC score train data: {}'.format(roc_auc_score(y_train,probs_train[:,1])))\n",
    "    print('ROC score test data: {}'.format(roc_auc_score(y_test,probs_test[:,1])))\n",
    "    \n",
    "    return roc_auc_score(y_test, probs_test[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a393f5",
   "metadata": {},
   "source": [
    "### train the different datasets and get the roc score using differnt under sampling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "21113451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_eval_34\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1728, 336]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(data\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# split into Train and Test data\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# call the Random Forest function\u001b[39;00m\n\u001b[0;32m     15\u001b[0m Random_Forest(X_train, X_test, y_train, y_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2422\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 378\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1728, 336]"
     ]
    }
   ],
   "source": [
    "roc_values = dict()\n",
    "\n",
    "for dataset in datasets_ls:\n",
    "    print(dataset)\n",
    "    dataset = fetch_datasets()[dataset]\n",
    "    columns = list(range(len(dataset.data[0][:])))\n",
    "    columns = list(map(str, columns))\n",
    "    X = pd.DataFrame(data = dataset.data, columns = columns)\n",
    "    y = pd.Series(data.target)\n",
    "    \n",
    "    # split into Train and Test data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    # call the Random Forest function\n",
    "    Random_Forest(X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
